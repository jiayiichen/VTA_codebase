{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b63ea2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dspy\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10748212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['discussion_post_id', 'discussion_post_content_clean',\n",
       "       'discussion_post_classification', 'discussion_topic', 'course_title',\n",
       "       'course_desc', 'context_independence', 'parent_discussion_post_id',\n",
       "       'llama_70b_ta_response',\n",
       "       'Clarify misunderstandings and address questions',\n",
       "       'Deepen Disciplinary Understanding', 'Develop Higher-Order Thinking',\n",
       "       'Enhance Metacognitive Awareness',\n",
       "       'Foster Collaborative Knowledge Construction and Social Presence'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = pandas.read_excel(\"300 revised.xlsx\")\n",
    "df_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "825c7d53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0.1', 'Unnamed: 0', 'discussion_post_id',\n",
       "       'discussion_post_content_clean', 'discussion_post_classification',\n",
       "       'discussion_topic', 'course_title', 'course_desc',\n",
       "       'context_independence', 'parent_discussion_post_id', 'mellon_id',\n",
       "       'discussion_context', 'llama_70b_ta_response_with_similar',\n",
       "       'Clarify misunderstandings and address questions',\n",
       "       'Deepen Disciplinary Understanding', 'Develop Higher-Order Thinking',\n",
       "       'Enhance Metacognitive Awareness',\n",
       "       'Foster Collaborative Knowledge Construction and Social Presence',\n",
       "       'Unnamed: 18', 'Unnamed: 19'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic_ann = pandas.read_csv(\"300_with_context_annotation.csv\")\n",
    "df_topic_ann.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607e70c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Literal\n",
    "\n",
    "class SimpleClassifier(dspy.Signature):\n",
    "    \"\"\"Given a discussion forum post and the corresponding teaching assistant response, classify how well this response adheres to pedagogical goals specified with a rubric. Provide your classification as integers.\"\"\"\n",
    "    discussion_forum_post = dspy.InputField(desc=\"The original student forum post.\")\n",
    "    ta_response = dspy.InputField(desc=\"The corresponding teaching assistant response.\")\n",
    "    pedagogical_rubric = dspy.InputField(desc=\"The pedagogical rubric you should follow when evaluating the response.\")\n",
    "    discussion_topic = dspy.InputField(desc=\"Description and instruction for the specific discussion topic for students.\")\n",
    "    course_info = dspy.InputField(desc=\"Information about the course.\")\n",
    "    rating: Literal[0, 1, 2, \"NA\"] = dspy.OutputField()\n",
    "\n",
    "class ComplexSelfReflector(dspy.Module):\n",
    "    def __init__(self, callbacks=None):\n",
    "        super().__init__(callbacks)\n",
    "        self.classifier = dspy.ChainOfThought(SimpleClassifier)\n",
    "    \n",
    "    def forward(self, discussion_forum_post, ta_response, pedagogical_rubric, discussion_topic, course_info):\n",
    "        og_rating = self.classifier(discussion_forum_post=discussion_forum_post,\n",
    "                                    ta_response=ta_response,\n",
    "                                    pedagogical_rubric=pedagogical_rubric,\n",
    "                                    discussion_topic=discussion_topic,\n",
    "                                    course_info=course_info).rating\n",
    "        return dspy.Prediction(rating=og_rating)\n",
    "\n",
    "classifier = ComplexSelfReflector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "574c7ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 150\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "# first set of selections\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "selector_1 = np.random.choice(300, 150, replace=False)\n",
    "selector_2 = np.random.choice(300, 150, replace=False)\n",
    "\n",
    "print(len(selector_1), len(selector_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bf71d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 150\n",
      "293 296\n"
     ]
    }
   ],
   "source": [
    "LEVEL_1 = open(\"rubrics/level_1.txt\").read()\n",
    "\n",
    "all_train_examples = []\n",
    "\n",
    "all_val_examples = []\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for i, row in df_data.iterrows():\n",
    "    post = row[\"discussion_post_content_clean\"]\n",
    "    resp = row[\"llama_70b_ta_response\"]\n",
    "    for col, desc in [\n",
    "        (\"Clarify misunderstandings and address questions\", LEVEL_1),\n",
    "    ]:\n",
    "        if pandas.notna(row[col]):\n",
    "            if pandas.notna(row[\"course_desc\"]):\n",
    "                course_info = row[\"course_title\"] + \"\\n\\n\" + row[\"course_desc\"]\n",
    "            else:\n",
    "                course_info = row[\"course_title\"]\n",
    "            new_ex = dspy.Example({\n",
    "                \"discussion_forum_post\": post,\n",
    "                \"ta_response\": resp,\n",
    "                \"pedagogical_rubric\": desc,\n",
    "                \"discussion_topic\": row[\"discussion_topic\"],\n",
    "                \"course_info\": course_info,\n",
    "                \"rating\": int(row[col]),\n",
    "                \"level\": col\n",
    "            }).with_inputs(\"discussion_forum_post\", \"ta_response\", \"pedagogical_rubric\", \"discussion_topic\", \"course_info\")\n",
    "            if counter not in selector_1:\n",
    "                all_train_examples.append(new_ex)\n",
    "            else:\n",
    "                all_val_examples.append(new_ex)\n",
    "    counter += 1\n",
    "\n",
    "print(len(all_train_examples), len(all_val_examples))\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for i, row in df_topic_ann.iterrows():\n",
    "    post = row[\"discussion_post_content_clean\"]\n",
    "    resp = row[\"llama_70b_ta_response_with_similar\"]\n",
    "    for col, desc in [\n",
    "        (\"Clarify misunderstandings and address questions\", LEVEL_1),\n",
    "    ]:\n",
    "        if pandas.notna(row[col]):\n",
    "            if pandas.notna(row[\"course_desc\"]):\n",
    "                course_info = row[\"course_title\"] + \"\\n\\n\" + row[\"course_desc\"]\n",
    "            else:\n",
    "                course_info = row[\"course_title\"]\n",
    "            new_ex = dspy.Example({\n",
    "                \"discussion_forum_post\": post,\n",
    "                \"ta_response\": resp,\n",
    "                \"pedagogical_rubric\": desc,\n",
    "                \"discussion_topic\": row[\"discussion_topic\"],\n",
    "                \"course_info\": course_info,\n",
    "                \"rating\": int(row[col]),\n",
    "                \"level\": col\n",
    "            }).with_inputs(\"discussion_forum_post\", \"ta_response\", \"pedagogical_rubric\", \"discussion_topic\", \"course_info\")\n",
    "            if counter not in selector_2:\n",
    "                all_train_examples.append(new_ex)\n",
    "            else:\n",
    "                all_val_examples.append(new_ex)\n",
    "    counter += 1\n",
    "\n",
    "train = all_train_examples\n",
    "val = all_val_examples\n",
    "print(len(train), len(val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6d41cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_gold_labels, all_pred_labels = {}, {}\n",
    "all_examples = {}\n",
    "\n",
    "def acc_metric(gold, pred, trace=None):\n",
    "    gold_rating = str(gold.rating)\n",
    "    pred_rating = str(pred.rating)\n",
    "    if gold_rating == \"3\":\n",
    "        gold_rating = \"NA\"\n",
    "    if gold.level not in all_gold_labels:\n",
    "        all_gold_labels.update({gold.level: []})\n",
    "        all_pred_labels.update({gold.level: []})\n",
    "        all_examples.update({gold.level: []})\n",
    "    all_gold_labels[gold.level].append(gold_rating)\n",
    "    all_pred_labels[gold.level].append(pred_rating)\n",
    "    all_examples[gold.level].append([gold.discussion_forum_post,\n",
    "                                     gold.ta_response])\n",
    "    fin_score = int(gold_rating == pred_rating)\n",
    "    if trace is not None: return fin_score == 1\n",
    "    return fin_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18ef4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv(\".env\")\n",
    "\n",
    "lm = dspy.LM(\"openai/gpt-4o\", cache=True, max_tokens=4000)\n",
    "dspy.configure(lm=lm)\n",
    "evaluate = dspy.Evaluate(metric=acc_metric, devset=val, nsum_threads=8, display_progress=True, display_table=5, max_errors=100, provide_traceback=True)\n",
    "eval_score = evaluate(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "094830bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_gold_labels), len(all_pred_labels), len(all_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6101ad86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Clarify misunderstandings and address questions'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_gold_labels.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3d5bb086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clarify misunderstandings and address questions\n",
      "(0.8397398234354756, 0.875, 0.8562545291070613, None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sylvia/anaconda3/envs/papillon/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "for k in all_gold_labels:\n",
    "    print(k)\n",
    "    print(precision_recall_fscore_support(y_true=all_gold_labels[k],\n",
    "                                          y_pred=all_pred_labels[k],\n",
    "                                          average=\"weighted\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e93ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "for k in all_gold_labels:\n",
    "    print(k)\n",
    "    print(Counter(all_gold_labels[k]))\n",
    "    print(precision_recall_fscore_support(y_true=all_gold_labels[k],\n",
    "                                          y_pred=all_pred_labels[k],\n",
    "                                          average=\"weighted\"))\n",
    "\n",
    "    cm = confusion_matrix(all_gold_labels[k], all_pred_labels[k], labels=[\"0\", \"1\", \"2\", \"NA\"])\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                                display_labels=[\"0\", \"1\", \"2\", \"NA\"])\n",
    "    disp.plot()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983fe6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    kwargs = dict(num_threads=8, display_progress=True, display_table=0)\n",
    "    \n",
    "\n",
    "    teleprompter = dspy.SIMBA(metric=acc_metric, max_demos=10)\n",
    "    \n",
    "    compiled_prompt_opt = teleprompter.compile(classifier, trainset=train)\n",
    "    evaluate = dspy.Evaluate(metric=acc_metric, devset=val, nsum_threads=8, display_progress=True, display_table=5, max_errors=100, provide_traceback=True)\n",
    "    eval_score = evaluate(compiled_prompt_opt)\n",
    "    print(eval_score)\n",
    "    \n",
    "    compiled_prompt_opt.save(f\"optimized_prompt_4o_level_1.json\")\n",
    "except ValueError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eab38c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_gold_labels, all_pred_labels = {}, {}\n",
    "all_examples = {}\n",
    "evaluate = dspy.Evaluate(metric=acc_metric, devset=val, nsum_threads=8, display_progress=True, display_table=5, max_errors=100, provide_traceback=True)\n",
    "eval_score = evaluate(compiled_prompt_opt)\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "for k in all_gold_labels:\n",
    "    print(k)\n",
    "    print(precision_recall_fscore_support(y_true=all_gold_labels[k],\n",
    "                                          y_pred=all_pred_labels[k],\n",
    "                                          average=\"weighted\"))\n",
    "    print(accuracy_score(y_true=all_gold_labels[k], y_pred=all_pred_labels[k]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "papillon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
